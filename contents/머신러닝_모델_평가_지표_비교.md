## 머신러닝의 Precision과 Recall score의 차이점 

### Precision과 Recall: 머신러닝 모델 평가의 핵심 지표

Precision과 Recall은 머신러닝 모델의 성능을 평가하는 데 있어 매우 중요한 지표입니다. 이들은 모델이 얼마나 정확하게 예측하는지, 그리고 실제 긍정 사례를 얼마나 잘 포착하는지를 측정합니다.

Precision은 모델이 긍정으로 예측한 사례 중에서 실제로 긍정인 비율을 나타냅니다. 예를 들어 스팸 메일 탐지 모델에서 Precision이 0.9라면, 모델이 스팸으로 분류한 메일 중 90%가 실제 스팸 메일이라는 의미입니다. 반면 Recall은 실제 긍정 사례 중에서 모델이 긍정으로 예측한 비율을 의미합니다. 스팸 메일 탐지 모델에서 Recall이 0.8이라면 실제 스팸 메일의 80%를 모델이 스팸으로 정확히 분류했다는 뜻입니다.

Precision과 Recall은 서로 트레이드오프 관계에 있어, 한쪽을 높이면 다른 쪽이 낮아지는 경향이 있습니다. 이 때문에 모델 평가에서는 두 지표를 모두 고려해야 합니다. 이를 하나의 지표로 종합한 것이 F1 스코어인데, 이는 Precision과 Recall의 조화평균으로 계산됩니다. F1 스코어가 높을수록 모델의 전반적인 성능이 우수하다고 볼 수 있습니다.

요컨대 Precision, Recall, F1 스코어는 머신러닝 모델의 성능을 다각도로 평가할 수 있게 해주는 핵심 지표입니다. 특히 불균형한 데이터셋에서는 정확도만으로 모델을 평가하기 어려우므로, 이들 지표를 활용하는 것이 중요합니다.


#### 이진 분류 문제에서의 Precision과 Recall 계산

이진 분류 문제에서 Precision과 Recall은 모델의 성능을 평가하는 중요한 지표입니다. 이를 계산하기 위해서는 먼저 혼동 행렬(confusion matrix)에 대해 알아야 합니다. 혼동 행렬은 모델의 예측 결과와 실제 값을 비교하여 나타낸 표입니다. 이진 분류 문제에서의 혼동 행렬은 다음과 같이 구성됩니다:

|                | 실제 긍정 | 실제 부정 |
|---------------:|:----------:|:----------:|
| **예측 긍정** |     TP     |     FP     |
| **예측 부정** |     FN     |     TN     |

- TP (True Positive): 실제 긍정이며 모델이 긍정으로 예측한 경우
- FP (False Positive): 실제 부정이지만 모델이 긍정으로 잘못 예측한 경우
- FN (False Negative): 실제 긍정이지만 모델이 부정으로 잘못 예측한 경우
- TN (True Negative): 실제 부정이며 모델이 부정으로 예측한 경우

Precision과 Recall의 공식은 다음과 같습니다:

- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)

Precision은 모델이 긍정으로 예측한 사례 중에서 실제 긍정인 비율을 나타내며, Recall은 실제 긍정 사례 중에서 모델이 긍정으로 예측한 비율입니다. 일반적으로 Precision과 Recall 사이에는 트레이드오프 관계가 있어, 한 지표를 높이면 다른 지표는 낮아지는 경향이 있습니다. 따라서 목적에 맞게 적절한 균형을 잡는 것이 중요합니다.

예를 들어 스팸 메일 탐지 모델에서 TP=90, FP=10, FN=20이라면 Precision은 0.9(90%), Recall은 0.818(81.8%)입니다. 즉, 모델이 스팸 메일로 예측한 경우 90%는 실제 스팸 메일이었고, 실제 스팸 메일의 81.8%를 정확히 예측했습니다. 이처럼 Precision과 Recall은 모델의 서로 다른 측면을 평가하므로, 두 지표를 모두 고려해야 모델의 전반적인 성능을 파악할 수 있습니다.

Precision과 Recall의 트레이드오프를 조정하는 방법 중 하나는 분류 결정 임계값을 조정하는 것입니다. 임계값을 높이면 Precision은 높아지고 Recall은 낮아지며, 반대로 임계값을 낮추면 Precision은 낮아지고 Recall은 높아집니다. 따라서 목적에 따라 적절한 임계값을 선택하는 것이 중요합니다.


#### 다중 클래스 분류 문제에서의 Precision과 Recall

다중 클래스 분류 문제에서 Precision과 Recall은 각 클래스별로 계산됩니다. 먼저 혼동 행렬을 확장하여 모든 클래스에 대한 예측 결과를 표현합니다. 예를 들어 3개의 클래스 A, B, C가 있다면 혼동 행렬은 다음과 같이 구성됩니다:

|                | 실제 A | 실제 B | 실제 C |
|---------------:|:-------:|:-------:|:-------:|
| **예측 A**    |   TP_A  |   FP_A  |   FP_A  |
| **예측 B**    |   FN_B  |   TP_B  |   FP_B  |
| **예측 C**    |   FN_C  |   FP_C  |   TP_C  |

이제 각 클래스별 Precision과 Recall을 계산할 수 있습니다:

- Precision_A = TP_A / (TP_A + FP_A + FP_A)
- Recall_A = TP_A / (TP_A + FN_B + FN_C)
- Precision_B = TP_B / (FN_B + TP_B + FP_B)
- Recall_B = TP_B / (FP_A + TP_B + FN_C)
- Precision_C = TP_C / (FN_C + FP_C + TP_C)
- Recall_C = TP_C / (FP_A + FP_B + TP_C)

예를 들어 고양이, 개, 자동차 3개 클래스 분류 문제에서 고양이 클래스의 Precision과 Recall은 다음과 같이 계산됩니다:

- Precision_고양이 = (고양이라고 예측한 개체 중 실제 고양이 수) / (고양이라고 예측한 수)
- Recall_고양이 = (실제 고양이 중 고양이라고 예측한 수) / (실제 고양이 수)

이처럼 다중 클래스 분류 문제에서는 각 클래스별로 Precision과 Recall을 계산해야 합니다. 이진 분류 문제와 달리, 한 클래스의 FP는 다른 클래스의 FN이 됩니다. 따라서 각 클래스의 Precision과 Recall은 서로 영향을 미치게 됩니다. 전체 Precision과 Recall도 계산할 수 있지만, 각 클래스의 중요도를 동일하게 가정하므로 실제 문제에 따라 적절하지 않을 수 있습니다. 그래서 다중 클래스 분류 문제에서는 각 클래스별 Precision과 Recall을 고려하는 것이 더 바람직합니다.


#### 정밀도(Precision)와 재현율(Recall) 사이의 트레이드오프 관계

정밀도와 재현율은 분류 모델의 성능을 평가하는 중요한 지표입니다. 하지만 이 두 지표 사이에는 일반적으로 상충관계가 존재합니다. 즉, 정밀도를 높이면 재현율이 낮아지고, 반대로 재현율을 높이면 정밀도가 낮아지는 경향이 있습니다. 이러한 트레이드오프 관계는 모델의 예측 임계값(threshold)을 조정함으로써 발생합니다.

예를 들어 이진 분류 문제에서 모델의 예측 확률 임계값을 높이면 긍정 클래스로 예측되는 경우가 줄어들어 정밀도는 높아지지만, 실제 긍정 사례 중 많은 부분을 놓치게 되어 재현율은 낮아집니다. 반대로 임계값을 낮추면 재현율은 높아지지만 정밀도는 낮아집니다.

이러한 정밀도-재현율 트레이드오프를 시각화하는 방법 중 하나가 정밀도-재현율 곡선(Precision-Recall Curve)입니다. 이 곡선은 x축에 재현율, y축에 정밀도를 놓고 그린 그래프로, 모델의 예측 임계값 변화에 따른 정밀도와 재현율의 변화를 보여줍니다. 일반적으로 곡선은 왼쪽 상단에서 시작하여 오른쪽 하단으로 내려오는 모양을 가집니다.

정밀도-재현율 곡선 아래의 면적(Average Precision 또는 Area Under the Precision-Recall Curve)을 계산하여 모델의 성능을 평가할 수 있습니다. 이 값이 클수록 모델의 성능이 좋다고 볼 수 있습니다.

특히 불균형 데이터셋(imbalanced dataset)에서 정밀도-재현율 곡선은 매우 유용합니다. 불균형 데이터셋이란 클래스 간 샘플 수의 차이가 큰 데이터셋을 말하며, 이런 경우 정밀도와 재현율을 모두 고려하는 것이 중요합니다. 정밀도-재현율 곡선은 이러한 불균형 데이터셋에서 모델의 성능을 평가하는 데 유용한 도구가 됩니다.


#### 머신러닝 모델 평가 지표: Precision, Recall 및 다른 지표들

머신러닝 모델의 성능을 평가하는 데 있어 Precision과 Recall은 매우 중요한 지표입니다. Precision은 모델이 긍정으로 예측한 사례 중 실제 긍정인 비율을 나타내며, Recall은 실제 긍정 사례 중 모델이 긍정으로 예측한 비율을 보여줍니다. 이들 지표는 모델의 예측 성능을 직관적으로 이해할 수 있게 해주고, 불균형 데이터셋에서도 유용하게 사용될 수 있습니다.

Precision = TP / (TP + FP)
Recall = TP / (TP + FN)

여기서 TP(True Positive)는 실제 긍정을 긍정으로 예측한 경우, FP(False Positive)는 실제 부정을 긍정으로 잘못 예측한 경우, FN(False Negative)은 실제 긍정을 부정으로 잘못 예측한 경우를 의미합니다.

예를 들어 스팸 메일 탐지 모델에서 Precision이 높다면 모델이 스팸으로 예측한 메일 중 실제 스팸 메일의 비율이 높음을 의미합니다. 반면 Recall이 높다면 모델이 실제 스팸 메일을 스팸으로 잘 예측함을 뜻합니다. 이 경우 Precision이 더 중요할 수 있습니다.

하지만 Precision과 Recall은 서로 트레이드오프 관계에 있어, 한 지표를 높이면 다른 지표는 낮아지는 경향이 있습니다. 이를 보완하기 위해 F1 스코어(F1 = 2 * (Precision * Recall) / (Precision + Recall))가 사용됩니다. F1 스코어는 Precision과 Recall의 조화평균으로, 두 지표의 균형을 고려합니다.

이 외에도 정확도(Accuracy), ROC 곡선(Receiver Operating Characteristic Curve), AUC(Area Under the Curve) 등의 평가 지표가 있습니다. 정확도는 전체 예측 중 올바르게 예측한 비율을 나타내며, ROC 곡선과 AUC는 불균형 데이터셋에서 유용하게 사용될 수 있습니다.

결론적으로 Precision, Recall, F1 스코어 등 다양한 평가 지표를 종합적으로 고려하여 모델의 성능을 다각도로 분석하는 것이 중요합니다. 특히 불균형 데이터셋에서는 Precision, Recall, F1 스코어뿐만 아니라 ROC 곡선과 AUC 등의 지표도 함께 살펴보는 것이 바람직합니다.


#### Precision과 Recall 개선을 위한 기법

Precision과 Recall은 기계 학습 모델의 성능을 평가하는 중요한 지표입니다. Precision은 모델이 양성으로 예측한 데이터 중 실제 양성인 비율을 나타내며, Recall은 실제 양성 데이터 중 모델이 양성으로 올바르게 예측한 비율을 의미합니다. 이들 지표는 특히 불균형 데이터셋이나 오류 비용이 다른 문제에서 중요합니다. 예를 들어 암 진단 문제에서는 실제 암 환자를 건강한 사람으로 잘못 분류하는 것(False Negative)이 큰 비용이 발생하므로 Recall이 더 중요할 수 있습니다.

Precision과 Recall 사이에는 트레이드오프 관계가 있어, 한 지표를 높이면 다른 지표가 낮아지는 경향이 있습니다. 따라서 문제의 특성과 목적에 맞게 적절한 균형을 잡는 것이 중요합니다. 이를 위해 다음과 같은 기법들을 활용할 수 있습니다.

1. 데이터 전처리 및 증강: 데이터의 품질을 개선하여 모델 성능을 높일 수 있습니다. 예를 들어 이미지 데이터에서 노이즈를 제거하거나 회전, 크기 조정 등의 증강 기법을 적용할 수 있습니다.

2. 모델 아키텍처 및 하이퍼파라미터 튜닝: 적절한 모델 아키텍처와 하이퍼파라미터 설정은 모델 성능에 큰 영향을 미칩니다. 예를 들어 CNN은 이미지 분류에, RNN은 시퀀스 데이터에 적합합니다.

3. 앙상블 기법: 여러 개의 모델을 결합하여 단점을 보완하고 강점을 결합할 수 있습니다. 대표적인 기법으로 배깅, 부스팅, 스태킹 등이 있습니다.

4. 임계값 조정: 모델의 예측 임계값을 조정하여 Precision과 Recall의 균형을 맞출 수 있습니다. 임계값을 낮추면 Recall이 높아지고, 높이면 Precision이 높아집니다.

5. 비용 민감 학습: 오류 비용을 고려하여 Precision과 Recall의 균형을 조정할 수 있습니다. 예를 들어 False Negative 비용이 높다면 Recall에 더 큰 가중치를 줄 수 있습니다.

6. 능동 학습: 모델이 불확실성이 높은 데이터를 선택하여 사람에게 레이블을 요청하고, 이를 통해 모델을 재학습하면 성능이 향상될 수 있습니다.

7. F-score 활용: Precision과 Recall의 가중 조화 평균인 F-score를 활용하여 두 지표의 균형을 평가할 수 있습니다. 가중치를 조정하여 F0.5, F2 등으로 Precision 또는 Recall에 더 큰 비중을 둘 수 있습니다.

8. Precision-Recall 곡선 분석: Precision-Recall 곡선을 통해 임계값 변화에 따른 Precision과 Recall의 변화를 시각화하고, 적절한 임계값을 선택할 수 있습니다.

이처럼 다양한 기법을 적절히 활용하여 Precision과 Recall을 개선할 수 있습니다. 문제의 특성과 목적에 맞게 두 지표의 균형을 잡는 것이 중요하며, 이를 통해 모델의 전반적인 성능을 향상시킬 수 있습니다.


